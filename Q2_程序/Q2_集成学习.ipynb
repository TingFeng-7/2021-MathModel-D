{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mathematical-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm 、 xgboost 、多元回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjustable-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "#from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exceptional-saturday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1974, 17)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('./pickle/ult_20_dummy.pkl')\n",
    "#data = pd.read_pickle('./pickle/xgbr_20_dummy.pkl')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coordinate-barcelona",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ETA_Beta</th>\n",
       "      <th>ETA_EtaP_B_RC</th>\n",
       "      <th>nHBAcc_Lipinski</th>\n",
       "      <th>ETA_BetaP</th>\n",
       "      <th>C2SP2</th>\n",
       "      <th>ATSc5</th>\n",
       "      <th>maxaaO</th>\n",
       "      <th>ATSc1</th>\n",
       "      <th>C1SP2</th>\n",
       "      <th>ECCEN</th>\n",
       "      <th>mindssC</th>\n",
       "      <th>ATSc2</th>\n",
       "      <th>ETA_EtaP_B</th>\n",
       "      <th>minaaO</th>\n",
       "      <th>nTG12Ring</th>\n",
       "      <th>ATSp4</th>\n",
       "      <th>IC50_nM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.50</td>\n",
       "      <td>0.01887</td>\n",
       "      <td>4</td>\n",
       "      <td>1.04839</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.080148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.460175</td>\n",
       "      <td>0</td>\n",
       "      <td>912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.217040</td>\n",
       "      <td>0.00500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4128.339568</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.50</td>\n",
       "      <td>0.01772</td>\n",
       "      <td>4</td>\n",
       "      <td>1.01515</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.080148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.460175</td>\n",
       "      <td>0</td>\n",
       "      <td>976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.217040</td>\n",
       "      <td>0.00469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4811.560374</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.25</td>\n",
       "      <td>0.02094</td>\n",
       "      <td>5</td>\n",
       "      <td>1.21970</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.059065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519350</td>\n",
       "      <td>0</td>\n",
       "      <td>977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.236031</td>\n",
       "      <td>0.00791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4479.177597</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.50</td>\n",
       "      <td>0.01823</td>\n",
       "      <td>4</td>\n",
       "      <td>1.01515</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.079743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.460206</td>\n",
       "      <td>0</td>\n",
       "      <td>983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.216970</td>\n",
       "      <td>0.00520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4311.294610</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.50</td>\n",
       "      <td>0.01823</td>\n",
       "      <td>4</td>\n",
       "      <td>1.19697</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.069664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462912</td>\n",
       "      <td>0</td>\n",
       "      <td>983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.215099</td>\n",
       "      <td>0.00520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4265.064807</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ETA_Beta  ETA_EtaP_B_RC  nHBAcc_Lipinski  ETA_BetaP  C2SP2     ATSc5  \\\n",
       "0     32.50        0.01887                4    1.04839     11 -0.080148   \n",
       "1     33.50        0.01772                4    1.01515     11 -0.080148   \n",
       "2     40.25        0.02094                5    1.21970     16 -0.059065   \n",
       "3     33.50        0.01823                4    1.01515     11 -0.079743   \n",
       "4     39.50        0.01823                4    1.19697     16 -0.069664   \n",
       "\n",
       "   maxaaO     ATSc1  C1SP2  ECCEN  mindssC     ATSc2  ETA_EtaP_B  minaaO  \\\n",
       "0     0.0  0.460175      0    912      0.0 -0.217040     0.00500     0.0   \n",
       "1     0.0  0.460175      0    976      0.0 -0.217040     0.00469     0.0   \n",
       "2     0.0  0.519350      0    977      0.0 -0.236031     0.00791     0.0   \n",
       "3     0.0  0.460206      0    983      0.0 -0.216970     0.00520     0.0   \n",
       "4     0.0  0.462912      0    983      0.0 -0.215099     0.00520     0.0   \n",
       "\n",
       "   nTG12Ring        ATSp4  IC50_nM  \n",
       "0          0  4128.339568      2.5  \n",
       "1          0  4811.560374      7.5  \n",
       "2          0  4479.177597      3.1  \n",
       "3          0  4311.294610      3.9  \n",
       "4          0  4265.064807      7.4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "moderate-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['IC50_nM']\n",
    "X = data.drop(labels='IC50_nM',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "forward-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "\n",
    "def evaluate(data,model):\n",
    "    pre = model.predict(data)\n",
    "    pre_res = [round(res) for res in pre]\n",
    "    y = data.get_label()\n",
    "    acc = r2_score(y,pre_res)\n",
    "    return acc\n",
    "#先定义评价函数\n",
    "def print_r2_score(model,x_train,y_train,x_test,y_test):\n",
    "    print(model.score(x_train,y_train))\n",
    "    print(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "floppy-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.120148</td>\n",
       "      <td>0.497765</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.721733</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.805280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050850</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.484430</td>\n",
       "      <td>0.941902</td>\n",
       "      <td>0.523012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.245841</td>\n",
       "      <td>0.402729</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.778696</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.831449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094339</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.036858</td>\n",
       "      <td>0.235369</td>\n",
       "      <td>0.908587</td>\n",
       "      <td>0.376899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171904</td>\n",
       "      <td>0.436133</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.903678</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.840253</td>\n",
       "      <td>0.881474</td>\n",
       "      <td>0.064187</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.014426</td>\n",
       "      <td>0.484430</td>\n",
       "      <td>0.942615</td>\n",
       "      <td>0.307194</td>\n",
       "      <td>0.881474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147874</td>\n",
       "      <td>0.357328</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.898328</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.795741</td>\n",
       "      <td>0.879982</td>\n",
       "      <td>0.065751</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.484430</td>\n",
       "      <td>0.965987</td>\n",
       "      <td>0.293566</td>\n",
       "      <td>0.879982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221811</td>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.476722</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.814901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052258</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.081260</td>\n",
       "      <td>0.786602</td>\n",
       "      <td>0.954832</td>\n",
       "      <td>0.134718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.120148  0.497765  0.046154  0.721733  0.500000  0.805280  0.000000   \n",
       "1  0.245841  0.402729  0.061538  0.778696  0.791667  0.831449  0.000000   \n",
       "2  0.171904  0.436133  0.046154  0.903678  0.541667  0.840253  0.881474   \n",
       "3  0.147874  0.357328  0.046154  0.898328  0.458333  0.795741  0.879982   \n",
       "4  0.221811  0.050576  0.030769  0.476722  0.625000  0.814901  0.000000   \n",
       "\n",
       "         7     8         9         10        11        12        13   14  \\\n",
       "0  0.050850  0.00  0.007742  0.484430  0.941902  0.523012  0.000000  0.0   \n",
       "1  0.094339  0.05  0.036858  0.235369  0.908587  0.376899  0.000000  0.0   \n",
       "2  0.064187  0.05  0.014426  0.484430  0.942615  0.307194  0.881474  0.0   \n",
       "3  0.065751  0.00  0.010720  0.484430  0.965987  0.293566  0.879982  0.0   \n",
       "4  0.052258  0.00  0.081260  0.786602  0.954832  0.134718  0.000000  0.0   \n",
       "\n",
       "         15  \n",
       "0  0.144257  \n",
       "1  0.230067  \n",
       "2  0.134747  \n",
       "3  0.075320  \n",
       "4  0.205111  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标准化 0.80\n",
    "std1 = MinMaxScaler()\n",
    "X=std1.fit_transform(X)\n",
    "std2 = MinMaxScaler()\n",
    "y=std2.fit_transform(y.values.reshape(-1,1))\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=666)\n",
    "pd.DataFrame(x_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-tuner",
   "metadata": {},
   "source": [
    "### 尝试线性回归和梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "generous-newcastle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wtf\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.820526262290205\n",
      "-65.23968693093067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(gamma=100)\n",
    "svr.fit(x_train,y_train)\n",
    "print_r2_score(svr,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-shelf",
   "metadata": {},
   "source": [
    "## 3.xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "threatened-andrew",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.42351\tvalidation_1-rmse:0.42323\n",
      "[30]\tvalidation_0-rmse:0.00453\tvalidation_1-rmse:0.00641\n",
      "[60]\tvalidation_0-rmse:0.00193\tvalidation_1-rmse:0.00546\n",
      "[90]\tvalidation_0-rmse:0.00134\tvalidation_1-rmse:0.00543\n",
      "[120]\tvalidation_0-rmse:0.00093\tvalidation_1-rmse:0.00543\n",
      "[150]\tvalidation_0-rmse:0.00093\tvalidation_1-rmse:0.00543\n",
      "[180]\tvalidation_0-rmse:0.00093\tvalidation_1-rmse:0.00543\n",
      "[210]\tvalidation_0-rmse:0.00093\tvalidation_1-rmse:0.00543\n",
      "[240]\tvalidation_0-rmse:0.00093\tvalidation_1-rmse:0.00543\n",
      "[270]\tvalidation_0-rmse:0.00093\tvalidation_1-rmse:0.00543\n",
      "[299]\tvalidation_0-rmse:0.00093\tvalidation_1-rmse:0.00543\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold, cross_val_score as CVS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    " \n",
    "\n",
    "#1.sklearn实现法 100和6\n",
    "xgbr = XGBR(n_estimators=300,max_depth=6, n_jobs=-1,learning_rate=0.15)\n",
    "xgbr.fit(x_train, y_train,\n",
    "        eval_set=[(x_train, y_train), (x_test,y_test)],\n",
    "        eval_metric='rmse',\n",
    "        verbose=30)\n",
    "\n",
    "evals_result = xgbr.evals_result()#fit中有eval_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-needle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "subject-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.90783438e-04  8.13311199e-05  1.37727649e-04  1.35604688e-03\n",
      " -2.35089319e-04  2.17155972e-03  1.24644337e-03  3.23998654e-04\n",
      "  3.53625772e-04  6.19863218e-04  8.26995063e-04  4.78335831e-04\n",
      "  5.72205572e-05  3.55979224e-04 -3.58588914e-05  4.02661011e-04\n",
      "  9.25892848e-04  2.82533205e-04  2.33590580e-03  6.51868249e-05\n",
      " -1.19910124e-04  3.57971403e-05  1.89155247e-03  6.58434699e-04\n",
      "  3.44931777e-03  4.66162805e-03  1.44224497e-04  2.23601307e-03\n",
      "  2.17252504e-03  2.54573510e-03  3.09188128e-03  3.49467248e-03\n",
      "  3.58799309e-03  4.38154303e-03  4.47675167e-03 -5.95485908e-04\n",
      " -1.03116105e-03  1.51711877e-03  2.73735877e-05  2.73735877e-05\n",
      "  1.07415348e-04  1.07415348e-04  1.07415348e-04 -2.01786999e-04\n",
      "  1.07415348e-04  5.48444164e-04  4.19344404e-04  4.24737344e-04\n",
      "  2.81876390e-04  1.80842195e-04]\n"
     ]
    }
   ],
   "source": [
    "## 做预测反推\n",
    "test_data = pd.read_pickle('./pickle/50_test_17_dummy.pkl')\n",
    "\n",
    "test_y = test_data['IC50_nM']\n",
    "test_X = test_data.drop(labels='IC50_nM',axis=1)\n",
    "\n",
    "#test_y = std2.transform(test_y)\n",
    "test_X = std1.transform(test_X)\n",
    "\n",
    "pred_y = xgbr.predict(test_X)\n",
    "\n",
    "print(pred_y)\n",
    "#pred_inverse_y = std2.inverse_transform(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "successful-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inverse_y = std2.inverse_transform(pred_y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "million-president",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>667.788025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284.704895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>482.092743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4746.209961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-822.766602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0   667.788025\n",
       "1   284.704895\n",
       "2   482.092743\n",
       "3  4746.209961\n",
       "4  -822.766602"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(pred_inverse_y)\n",
    "ic50_df = pd.DataFrame(pred_inverse_y)\n",
    "ic50_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "amateur-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic50_df.columns = ['ic50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "seeing-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cal_pIC50(input):\n",
    "    pIC50 = []\n",
    "\n",
    "    for i in input['ic50']:\n",
    "        molar = i*(10**-9) # Converts nM to M\n",
    "        pIC50.append(-np.log10(molar))\n",
    "\n",
    "    input['pIC50'] = pIC50\n",
    "    x = input.drop('ic50', 1)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "faced-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-02918b863c1f>:8: RuntimeWarning: invalid value encountered in log10\n",
      "  pIC50.append(-np.log10(molar))\n"
     ]
    }
   ],
   "source": [
    "pIC50 = cal_pIC50(ic50_df)\n",
    "#pIC50.head(50)\n",
    "df3 = pd.concat([ic50_df,pIC50],axis=1)\n",
    "df3.to_csv('q2-test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-ground",
   "metadata": {},
   "source": [
    "## xgboost原生方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secondary-clear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:13:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"nfold\", \"obj\", \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:13:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"nfold\", \"obj\", \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:13:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"nfold\", \"obj\", \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.参数字典\n",
    "param1 = {'silent':True\n",
    "          ,'obj':'reg:linear'\n",
    "          ,\"subsample\":1\n",
    "          ,\"max_depth\":6\n",
    "          ,\"eta\":0.3\n",
    "          ,\"gamma\":0\n",
    "          ,\"lambda\":1\n",
    "          ,\"alpha\":0\n",
    "          ,\"nfold\":5}\n",
    "num_round = 300 #迭代300次\n",
    "dfull = xgb.DMatrix(X, y)\n",
    "cvresult1 = xgb.cv(param1, dfull, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "static-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#传入fig 保存成高清矢量图\n",
    "def save_svg(fig,name):\n",
    "    fig.savefig(name, dpi=600, format='svg',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "smooth-elimination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_ylim(top=0.5)\n",
    "ax.grid()\n",
    "ax.plot(range(1,301),cvresult1.iloc[:,0],c=\"red\",label=\"train,original\")\n",
    "ax.plot(range(1,301),cvresult1.iloc[:,2],c=\"orange\",label=\"test,original\")\n",
    "ax.legend(fontsize=\"xx-large\")\n",
    "\n",
    "plt.xlabel('迭代次数')\n",
    "plt.title('RMSE均方误差学习曲线')\n",
    "#plt.rcParams.update({'font.size': 11})\n",
    "plt.legend(loc='best')\n",
    "\n",
    "save_svg(plt,'Q2-rmse.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "starting-tuesday",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9991359634977866\n",
      "0.800877409508529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'validation_0': OrderedDict([('rmse',\n",
       "               [0.423506,\n",
       "                0.360093,\n",
       "                0.306186,\n",
       "                0.260359,\n",
       "                0.221403,\n",
       "                0.188286,\n",
       "                0.160135,\n",
       "                0.136205,\n",
       "                0.115859,\n",
       "                0.098568,\n",
       "                0.083866,\n",
       "                0.071376,\n",
       "                0.060755,\n",
       "                0.051737,\n",
       "                0.044067,\n",
       "                0.03756,\n",
       "                0.032026,\n",
       "                0.027327,\n",
       "                0.023344,\n",
       "                0.019961,\n",
       "                0.01709,\n",
       "                0.014666,\n",
       "                0.012613,\n",
       "                0.010879,\n",
       "                0.009413,\n",
       "                0.008182,\n",
       "                0.007152,\n",
       "                0.006298,\n",
       "                0.005585,\n",
       "                0.005005,\n",
       "                0.004532,\n",
       "                0.00415,\n",
       "                0.003842,\n",
       "                0.003597,\n",
       "                0.003392,\n",
       "                0.003231,\n",
       "                0.003106,\n",
       "                0.00301,\n",
       "                0.00286,\n",
       "                0.002789,\n",
       "                0.00274,\n",
       "                0.002666,\n",
       "                0.002626,\n",
       "                0.002594,\n",
       "                0.00251,\n",
       "                0.00247,\n",
       "                0.002434,\n",
       "                0.002403,\n",
       "                0.002363,\n",
       "                0.002311,\n",
       "                0.002242,\n",
       "                0.002154,\n",
       "                0.002129,\n",
       "                0.002075,\n",
       "                0.002065,\n",
       "                0.002039,\n",
       "                0.001993,\n",
       "                0.001975,\n",
       "                0.001954,\n",
       "                0.001942,\n",
       "                0.001933,\n",
       "                0.001922,\n",
       "                0.001908,\n",
       "                0.001878,\n",
       "                0.001825,\n",
       "                0.001797,\n",
       "                0.001791,\n",
       "                0.00177,\n",
       "                0.001744,\n",
       "                0.001732,\n",
       "                0.001721,\n",
       "                0.001709,\n",
       "                0.001693,\n",
       "                0.001664,\n",
       "                0.001656,\n",
       "                0.001605,\n",
       "                0.001587,\n",
       "                0.001556,\n",
       "                0.001548,\n",
       "                0.001538,\n",
       "                0.00151,\n",
       "                0.001502,\n",
       "                0.001494,\n",
       "                0.001477,\n",
       "                0.001465,\n",
       "                0.001427,\n",
       "                0.00139,\n",
       "                0.001376,\n",
       "                0.001359,\n",
       "                0.001354,\n",
       "                0.001344,\n",
       "                0.001324,\n",
       "                0.001307,\n",
       "                0.001256,\n",
       "                0.001218,\n",
       "                0.001204,\n",
       "                0.001177,\n",
       "                0.001175,\n",
       "                0.001148,\n",
       "                0.001125,\n",
       "                0.001123,\n",
       "                0.001116,\n",
       "                0.001095,\n",
       "                0.001076,\n",
       "                0.001056,\n",
       "                0.001036,\n",
       "                0.00103,\n",
       "                0.001017,\n",
       "                0.001016,\n",
       "                0.000996,\n",
       "                0.000987,\n",
       "                0.000976,\n",
       "                0.000967,\n",
       "                0.000965,\n",
       "                0.000963,\n",
       "                0.000948,\n",
       "                0.000946,\n",
       "                0.000934,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926,\n",
       "                0.000926])]),\n",
       " 'validation_1': OrderedDict([('rmse',\n",
       "               [0.423233,\n",
       "                0.359762,\n",
       "                0.305817,\n",
       "                0.259971,\n",
       "                0.221011,\n",
       "                0.187923,\n",
       "                0.159784,\n",
       "                0.135876,\n",
       "                0.115539,\n",
       "                0.098267,\n",
       "                0.083584,\n",
       "                0.071112,\n",
       "                0.06052,\n",
       "                0.051545,\n",
       "                0.043917,\n",
       "                0.037433,\n",
       "                0.031927,\n",
       "                0.027308,\n",
       "                0.023398,\n",
       "                0.020063,\n",
       "                0.017298,\n",
       "                0.014955,\n",
       "                0.013042,\n",
       "                0.011459,\n",
       "                0.01019,\n",
       "                0.009153,\n",
       "                0.008286,\n",
       "                0.007642,\n",
       "                0.007122,\n",
       "                0.006748,\n",
       "                0.006414,\n",
       "                0.006208,\n",
       "                0.006048,\n",
       "                0.005898,\n",
       "                0.005787,\n",
       "                0.005704,\n",
       "                0.005621,\n",
       "                0.005573,\n",
       "                0.005542,\n",
       "                0.005527,\n",
       "                0.005509,\n",
       "                0.005504,\n",
       "                0.005495,\n",
       "                0.005494,\n",
       "                0.005482,\n",
       "                0.005478,\n",
       "                0.005487,\n",
       "                0.00549,\n",
       "                0.005477,\n",
       "                0.005481,\n",
       "                0.005471,\n",
       "                0.005462,\n",
       "                0.005459,\n",
       "                0.005464,\n",
       "                0.005457,\n",
       "                0.005457,\n",
       "                0.005463,\n",
       "                0.005461,\n",
       "                0.005465,\n",
       "                0.00546,\n",
       "                0.005459,\n",
       "                0.005451,\n",
       "                0.005455,\n",
       "                0.005447,\n",
       "                0.005454,\n",
       "                0.005449,\n",
       "                0.005443,\n",
       "                0.005443,\n",
       "                0.005444,\n",
       "                0.005441,\n",
       "                0.005442,\n",
       "                0.005441,\n",
       "                0.005442,\n",
       "                0.005441,\n",
       "                0.005443,\n",
       "                0.005432,\n",
       "                0.005431,\n",
       "                0.005429,\n",
       "                0.005431,\n",
       "                0.00543,\n",
       "                0.005425,\n",
       "                0.005427,\n",
       "                0.005425,\n",
       "                0.005428,\n",
       "                0.005429,\n",
       "                0.005431,\n",
       "                0.005431,\n",
       "                0.005432,\n",
       "                0.00543,\n",
       "                0.005429,\n",
       "                0.005432,\n",
       "                0.005429,\n",
       "                0.005432,\n",
       "                0.005434,\n",
       "                0.005436,\n",
       "                0.005436,\n",
       "                0.005444,\n",
       "                0.005446,\n",
       "                0.005444,\n",
       "                0.005445,\n",
       "                0.005444,\n",
       "                0.005436,\n",
       "                0.005436,\n",
       "                0.005439,\n",
       "                0.00544,\n",
       "                0.005439,\n",
       "                0.005436,\n",
       "                0.005433,\n",
       "                0.005432,\n",
       "                0.005432,\n",
       "                0.005432,\n",
       "                0.005432,\n",
       "                0.00543,\n",
       "                0.005429,\n",
       "                0.005427,\n",
       "                0.005428,\n",
       "                0.005428,\n",
       "                0.005429,\n",
       "                0.00543,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429,\n",
       "                0.005429])])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_r2_score(xgbr,x_train,y_train,x_test,y_test)\n",
    "evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "naval-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wtf\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\wtf\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\wtf\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:179: UserWarning: Converting column-vector to 1d array\n",
      "  _log_warning('Converting column-vector to 1d array')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid_0's rmse: 0.0127193\tvalid_0's l2: 0.000161781\n",
      "[60]\tvalid_0's rmse: 0.0153672\tvalid_0's l2: 0.00023615\n",
      "[90]\tvalid_0's rmse: 0.0178474\tvalid_0's l2: 0.00031853\n",
      "[120]\tvalid_0's rmse: 0.0190917\tvalid_0's l2: 0.000364495\n",
      "[150]\tvalid_0's rmse: 0.019966\tvalid_0's l2: 0.000398639\n",
      "[180]\tvalid_0's rmse: 0.0205903\tvalid_0's l2: 0.000423962\n",
      "[210]\tvalid_0's rmse: 0.0212102\tvalid_0's l2: 0.000449873\n",
      "[240]\tvalid_0's rmse: 0.0215397\tvalid_0's l2: 0.000463958\n",
      "[270]\tvalid_0's rmse: 0.0215968\tvalid_0's l2: 0.000466423\n",
      "[300]\tvalid_0's rmse: 0.0222279\tvalid_0's l2: 0.00049408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(learning_rate=0.15, max_depth=6, n_estimators=300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "lgbm = LGBMRegressor(n_estimators=300,max_depth=6,learning_rate=0.15)\n",
    "lgbm.fit(x_train, y_train\n",
    "        , eval_set=[(x_test, y_test)]\n",
    "        , eval_metric='rmse',verbose=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "advanced-shame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8911873786862128\n",
      "-2.3376558742403004\n"
     ]
    }
   ],
   "source": [
    "print_r2_score(lgbm,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-shelter",
   "metadata": {},
   "source": [
    "### mmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "allied-coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-e409a4ca54cc>:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfc.fit(x_train,y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfc = RandomForestRegressor()\n",
    "rfc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "banner-dancing",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8172369318319841\n",
      "0.5757475312080509\n"
     ]
    }
   ],
   "source": [
    "print_r2_score(rfc,x_train,y_train,x_test,y_test) ##可以接受"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-annual",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
